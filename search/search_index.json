{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":""},{"location":"#welcome","title":"Welcome","text":""},{"location":"#kuberay","title":"KubeRay","text":"<p>We have moved all documentation to the ray-project/ray repository. Please refer to the Ray docs for the latest information. The ray-project/kuberay repository hosts the KubeRay source code and community information.</p> <p>KubeRay is a powerful, open-source Kubernetes operator that simplifies the deployment and management of Ray applications on Kubernetes. It offers several key components:</p> <p>KubeRay core: This is the official, fully-maintained component of KubeRay that provides three custom resource definitions, RayCluster, RayJob, and RayService. These resources are designed to help you run a wide range of workloads with ease.</p> <ul> <li> <p>RayCluster: KubeRay fully manages the lifecycle of RayCluster, including cluster creation/deletion, autoscaling, and ensuring fault tolerance.</p> </li> <li> <p>RayJob: With RayJob, KubeRay automatically creates a RayCluster and submits a job when the cluster is ready. You can also configure RayJob to automatically delete the RayCluster once the job finishes.</p> </li> <li> <p>RayService: RayService is made up of two parts: a RayCluster and a Ray Serve deployment graph. RayService offers zero-downtime upgrades for RayCluster and high availability.</p> </li> </ul> <p>KubeRay ecosystem: Some optional components.</p> <ul> <li> <p>Kubectl Plugin (Beta): Starting from KubeRay v1.3.0, you can use the <code>kubectl ray</code> plugin to simplify common workflows when deploying Ray on Kubernetes. If you aren\u2019t familiar with Kubernetes, this plugin simplifies running Ray on Kubernetes. See kubectl-plugin for more details.</p> </li> <li> <p>KubeRay APIServer (Alpha): It provides a layer of simplified configuration for KubeRay resources. The KubeRay API server is used internally by some organizations to back user interfaces for KubeRay resource management.</p> </li> <li> <p>KubeRay Dashboard (Experimental): Starting from KubeRay v1.4.0, we have introduced a new dashboard that enables users to view and manage KubeRay resources. While it is not yet production-ready, we welcome your feedback.</p> </li> </ul>"},{"location":"community/community/","title":"KubeRay Community Governance","text":"<p>There are 4 roles in the KubeRay community:</p> <ul> <li>Contributor: A contributor is a member who contributes to the KubeRay project through code, documentation, PR reviews, answering questions, etc.</li> <li>Triager: A triager has the permission to triage issues and PRs.</li> <li>Reviewer: A reviewer can review PRs but cannot merge them.</li> <li>Committer: A committer can merge PRs and is responsible for the KubeRay project.</li> </ul>"},{"location":"community/community/#mechanism","title":"Mechanism","text":""},{"location":"community/community/#becoming-a-contributor","title":"Becoming a Contributor","text":"<p>To become a KubeRay contributor, you can contribute to KubeRay through code, documentation, PR reviews, answering questions, and more. It's helpful to join the KubeRay channel <code>#kuberay-questions</code> on the Ray Slack workspace to get started. You can also read ray-project/kuberay#1059 for tips on getting involved in the KubeRay community.</p> <p>In addition, you can add the Ray / KubeRay OSS community Google calendar to your calendar and join the bi-weekly KubeRay community meetings.</p>"},{"location":"community/community/#becoming-a-triager","title":"Becoming a Triager","text":"<p>A triager helps manage issues and PRs by assigning them to the appropriate people and adding labels to categorize them. To become a triager, you should demonstrate a basic understanding of the overall KubeRay project so you can determine whether an issue or PR is valid and how to categorize it.</p>"},{"location":"community/community/#becoming-a-reviewer","title":"Becoming a Reviewer","text":"<p>To become a KubeRay reviewer, you should demonstrate a deep understanding of certain features or components of the KubeRay project. A reviewer should be able to review PRs and provide constructive feedback to help the author improve the PR. Ideally, committers should feel confident merging a PR if you have approved it.</p>"},{"location":"community/community/#becoming-a-committer","title":"Becoming a Committer","text":"<p>Nothing in this document is guaranteed, but it provides a clear path for contributors toward becoming committers.</p> <p>A committer is a trusted member of the KubeRay community with a long-term commitment who has the ownership to offload the community's workloads and make KubeRay thrive. To elaborate,</p> <ul> <li>Offload community\u2019s workloads: A committer should be able to deliver high-quality code which doesn\u2019t require a lot   of code reviews, uphold high-quality code reviews, answer questions on GitHub and Slack, and release new KubeRay   releases to offload the community\u2019s workloads.</li> <li>Make KubeRay thrive: There are two main indicators that define the vibrancy of the KubeRay community:   (1) active contributors and (2) user adoptions.</li> <li>Involve more contributors: A committer should be able to involve more contributors by providing mentorship or     reviewing PRs.</li> <li>Increase user adoptions: A committer should be able to find user pain points and unlock new use cases by     delivering new features, help users by answering questions and fixing bugs, author blogs, and give talks.</li> </ul> <p>If contributors commit to the above two areas with long-term contributions, they will be nominated as KubeRay committers. The following paragraphs provide methods and more details about how to contribute to the two areas, and the order does not indicate importance. A contributor doesn't need to contribute to all of them to be a committer, and nothing is strictly required to be a committer, but all of them will be considered together.</p> <ul> <li>Code contribution:</li> <li>Deliver high-quality code that committers are comfortable merging without a lot of back-and-forth on PR reviews.</li> <li>End-to-end ownership of new features is also important; that is, writing user guides and advertising new features     after PRs are merged to ensure users can actually use them in production.</li> <li>PR reviews:</li> <li>Uphold high-quality code reviews, and committers are confident in merging the PR if you approve it.</li> <li>Answer questions:</li> <li>Answer questions on Slack and GitHub issues to unblock users.</li> <li>Mentorship:</li> <li>Mentor contributors to be involved in the community.</li> <li>Advocacy:</li> <li>Improve KubeRay's awareness within the AI infrastructure and cloud-native communities by speaking at conferences and     writing blogs.</li> <li>KubeRay releases:</li> <li>Help KubeRay releases ensure high-quality and stable KubeRay releases.</li> <li>Understand user pain points and propose new projects:</li> <li>Chat with users to understand their pain points and propose new projects or documents to unlock new use cases.</li> </ul>"},{"location":"community/community/#contributors","title":"Contributors","text":""},{"location":"community/community/#committers","title":"Committers","text":"<ul> <li>@kevin85421</li> <li>@andrewsykim</li> <li>@rueian</li> <li>@MortalHappiness</li> </ul>"},{"location":"deploy/helm-cluster/","title":"RayCluster","text":"<p>A Helm chart for deploying the RayCluster with the kuberay operator.</p> <p>Homepage: https://github.com/ray-project/kuberay</p>"},{"location":"deploy/helm-cluster/#introduction","title":"Introduction","text":"<p>RayCluster is a custom resource definition (CRD). KubeRay operator will listen to the resource events about RayCluster and create related Kubernetes resources (e.g. Pod &amp; Service). Hence, KubeRay operator installation and CRD registration are required for this guide.</p>"},{"location":"deploy/helm-cluster/#prerequisites","title":"Prerequisites","text":"<p>See kuberay-operator/README.md for more details.</p> <ul> <li>Helm</li> <li>Install custom resource definition and KubeRay operator (covered by the following end-to-end   example.)</li> </ul>"},{"location":"deploy/helm-cluster/#end-to-end-example","title":"End-to-end example","text":"<pre><code># Step 1: Create a KinD cluster\nkind create cluster\n\n# Step 2: Register a Helm chart repo\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\n\n# Step 3: Install both CRDs and KubeRay operator v1.1.0.\nhelm install kuberay-operator kuberay/kuberay-operator --version 1.1.0\n\n# Step 4: Install a RayCluster custom resource\n# (For x86_64 users)\nhelm install raycluster kuberay/ray-cluster --version 1.1.0\n# (For arm64 users, e.g. Mac M1)\n# See here for all available arm64 images: https://hub.docker.com/r/rayproject/ray/tags?page=1&amp;name=aarch64\nhelm install raycluster kuberay/ray-cluster --version 1.1.0 --set image.tag=nightly-aarch64\n\n# Step 5: Verify the installation of KubeRay operator and RayCluster\nkubectl get pods\n# NAME                                          READY   STATUS    RESTARTS   AGE\n# kuberay-operator-6fcbb94f64-gkpc9             1/1     Running   0          89s\n# raycluster-kuberay-head-qp9f4                 1/1     Running   0          66s\n# raycluster-kuberay-worker-workergroup-2jckt   1/1     Running   0          66s\n\n# Step 6: Forward the port of Dashboard\nkubectl port-forward svc/raycluster-kuberay-head-svc 8265:8265\n\n# Step 7: Check 127.0.0.1:8265 for the Dashboard\n\n# Step 8: Log in to Ray head Pod and execute a job.\nkubectl exec -it ${RAYCLUSTER_HEAD_POD} -- bash\npython -c \"import ray; ray.init(); print(ray.cluster_resources())\" # (in Ray head Pod)\n\n# Step 9: Check 127.0.0.1:8265/#/job. The status of the job should be \"SUCCEEDED\".\n\n# Step 10: Uninstall RayCluster\nhelm uninstall raycluster\n\n# Step 11: Verify that RayCluster has been removed successfully\n# NAME                                READY   STATUS    RESTARTS   AGE\n# kuberay-operator-6fcbb94f64-gkpc9   1/1     Running   0          9m57s\n</code></pre>"},{"location":"deploy/helm-cluster/#values","title":"Values","text":"Key Type Default Description image.repository string <code>\"rayproject/ray\"</code> Image repository. image.tag string <code>\"2.46.0\"</code> Image tag. image.pullPolicy string <code>\"IfNotPresent\"</code> Image pull policy. nameOverride string <code>\"kuberay\"</code> String to partially override release name. fullnameOverride string <code>\"\"</code> String to fully override release name. imagePullSecrets list <code>[]</code> Secrets with credentials to pull images from a private registry gcsFaultTolerance.enabled bool <code>false</code> common.containerEnv list <code>[]</code> containerEnv specifies environment variables for the Ray head and worker containers. Follows standard K8s container env schema. head.initContainers list <code>[]</code> Init containers to add to the head pod head.labels object <code>{}</code> Labels for the head pod head.serviceAccountName string <code>\"\"</code> head.restartPolicy string <code>\"\"</code> head.containerEnv list <code>[]</code> head.envFrom list <code>[]</code> envFrom to pass to head pod head.resources.limits.cpu string <code>\"1\"</code> head.resources.limits.memory string <code>\"2G\"</code> head.resources.requests.cpu string <code>\"1\"</code> head.resources.requests.memory string <code>\"2G\"</code> head.annotations object <code>{}</code> Extra annotations for head pod head.nodeSelector object <code>{}</code> Node labels for head pod assignment head.tolerations list <code>[]</code> Node tolerations for head pod scheduling to nodes with taints head.affinity object <code>{}</code> Head pod affinity head.podSecurityContext object <code>{}</code> Head pod security context. head.securityContext object <code>{}</code> Ray container security context. head.volumes[0].name string <code>\"log-volume\"</code> head.volumes[0].emptyDir object <code>{}</code> head.volumeMounts[0].mountPath string <code>\"/tmp/ray\"</code> head.volumeMounts[0].name string <code>\"log-volume\"</code> head.sidecarContainers list <code>[]</code> head.command list <code>[]</code> head.args list <code>[]</code> head.headService object <code>{}</code> head.topologySpreadConstraints list <code>[]</code> head.rayStartParams object <code>{}</code> worker.groupName string <code>\"workergroup\"</code> The name of the workergroup worker.replicas int <code>1</code> The number of replicas for the worker pod worker.minReplicas int <code>1</code> The minimum number of replicas for the worker pod worker.maxReplicas int <code>3</code> The maximum number of replicas for the worker pod worker.labels object <code>{}</code> Labels for the worker pod worker.serviceAccountName string <code>\"\"</code> worker.restartPolicy string <code>\"\"</code> worker.initContainers list <code>[]</code> Init containers to add to the worker pod worker.containerEnv list <code>[]</code> worker.envFrom list <code>[]</code> envFrom to pass to worker pod worker.resources.limits.cpu string <code>\"1\"</code> worker.resources.limits.memory string <code>\"1G\"</code> worker.resources.requests.cpu string <code>\"1\"</code> worker.resources.requests.memory string <code>\"1G\"</code> worker.annotations object <code>{}</code> Extra annotations for worker pod worker.nodeSelector object <code>{}</code> Node labels for worker pod assignment worker.tolerations list <code>[]</code> Node tolerations for worker pod scheduling to nodes with taints worker.affinity object <code>{}</code> Worker pod affinity worker.podSecurityContext object <code>{}</code> Worker pod security context. worker.securityContext object <code>{}</code> Ray container security context. worker.volumes[0].name string <code>\"log-volume\"</code> worker.volumes[0].emptyDir object <code>{}</code> worker.volumeMounts[0].mountPath string <code>\"/tmp/ray\"</code> worker.volumeMounts[0].name string <code>\"log-volume\"</code> worker.sidecarContainers list <code>[]</code> worker.command list <code>[]</code> worker.args list <code>[]</code> worker.topologySpreadConstraints list <code>[]</code> worker.rayStartParams object <code>{}</code> additionalWorkerGroups.smallGroup.disabled bool <code>true</code> additionalWorkerGroups.smallGroup.replicas int <code>0</code> The number of replicas for the additional worker pod additionalWorkerGroups.smallGroup.minReplicas int <code>0</code> The minimum number of replicas for the additional worker pod additionalWorkerGroups.smallGroup.maxReplicas int <code>3</code> The maximum number of replicas for the additional worker pod additionalWorkerGroups.smallGroup.labels object <code>{}</code> Labels for the additional worker pod additionalWorkerGroups.smallGroup.serviceAccountName string <code>\"\"</code> additionalWorkerGroups.smallGroup.restartPolicy string <code>\"\"</code> additionalWorkerGroups.smallGroup.containerEnv list <code>[]</code> additionalWorkerGroups.smallGroup.envFrom list <code>[]</code> envFrom to pass to additional worker pod additionalWorkerGroups.smallGroup.resources.limits.cpu int <code>1</code> additionalWorkerGroups.smallGroup.resources.limits.memory string <code>\"1G\"</code> additionalWorkerGroups.smallGroup.resources.requests.cpu int <code>1</code> additionalWorkerGroups.smallGroup.resources.requests.memory string <code>\"1G\"</code> additionalWorkerGroups.smallGroup.annotations object <code>{}</code> Extra annotations for additional worker pod additionalWorkerGroups.smallGroup.nodeSelector object <code>{}</code> Node labels for additional worker pod assignment additionalWorkerGroups.smallGroup.tolerations list <code>[]</code> Node tolerations for additional worker pod scheduling to nodes with taints additionalWorkerGroups.smallGroup.affinity object <code>{}</code> Additional worker pod affinity additionalWorkerGroups.smallGroup.podSecurityContext object <code>{}</code> Additional worker pod security context. additionalWorkerGroups.smallGroup.securityContext object <code>{}</code> Ray container security context. additionalWorkerGroups.smallGroup.volumes[0].name string <code>\"log-volume\"</code> additionalWorkerGroups.smallGroup.volumes[0].emptyDir object <code>{}</code> additionalWorkerGroups.smallGroup.volumeMounts[0].mountPath string <code>\"/tmp/ray\"</code> additionalWorkerGroups.smallGroup.volumeMounts[0].name string <code>\"log-volume\"</code> additionalWorkerGroups.smallGroup.sidecarContainers list <code>[]</code> additionalWorkerGroups.smallGroup.command list <code>[]</code> additionalWorkerGroups.smallGroup.args list <code>[]</code> additionalWorkerGroups.smallGroup.topologySpreadConstraints list <code>[]</code> additionalWorkerGroups.smallGroup.rayStartParams object <code>{}</code> service.type string <code>\"ClusterIP\"</code>"},{"location":"deploy/helm/","title":"kuberay-operator","text":"<p>A Helm chart for deploying the Kuberay operator on Kubernetes.</p> <p>Homepage: https://github.com/ray-project/kuberay</p>"},{"location":"deploy/helm/#introduction","title":"Introduction","text":"<p>This document provides instructions to install both CRDs (RayCluster, RayJob, RayService) and KubeRay operator with a Helm chart.</p>"},{"location":"deploy/helm/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes</li> <li>Helm &gt;= 3</li> </ul> <p>Make sure the version of Helm is v3+. Currently, existing CI tests are based on Helm v3.4.1 and v3.9.4.</p> <pre><code>helm version\n</code></pre>"},{"location":"deploy/helm/#install-crds-and-kuberay-operator","title":"Install CRDs and KubeRay operator","text":"<ul> <li>Install a stable version via Helm repository (only supports KubeRay v0.4.0+)</li> </ul> <pre><code>helm repo add kuberay https://ray-project.github.io/kuberay-helm/\n\n# Install both CRDs and KubeRay operator v1.1.0.\nhelm install kuberay-operator kuberay/kuberay-operator --version 1.1.0\n\n# Check the KubeRay operator Pod in `default` namespace\nkubectl get pods\n# NAME                                READY   STATUS    RESTARTS   AGE\n# kuberay-operator-6fcbb94f64-mbfnr   1/1     Running   0          17s\n</code></pre> <ul> <li>Install the nightly version</li> </ul> <pre><code># Step1: Clone KubeRay repository\n\n# Step2: Move to `helm-chart/kuberay-operator`\n\n# Step3: Install KubeRay operator\nhelm install kuberay-operator .\n</code></pre> <ul> <li>Install KubeRay operator without installing CRDs</li> <li>In some cases, the installation of the CRDs and the installation of the operator may require     different levels of admin permissions, so these two installations could be handled as different     steps by different roles.</li> <li>Use Helm's built-in <code>--skip-crds</code> flag to install the operator only.     See this document for more details.</li> </ul> <pre><code># Step 1: Install CRDs only (for cluster admin)\nkubectl create -k \"github.com/ray-project/kuberay/ray-operator/config/crd?ref=v1.1.0&amp;timeout=90s\"\n\n# Step 2: Install KubeRay operator only. (for developer)\nhelm install kuberay-operator kuberay/kuberay-operator --version 1.1.0 --skip-crds\n</code></pre>"},{"location":"deploy/helm/#list-the-chart","title":"List the chart","text":"<p>To list the <code>my-release</code> deployment:</p> <pre><code>helm ls\n# NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION\n# kuberay-operator        default         1               2023-09-22 02:57:17.306616331 +0000 UTC deployed        kuberay-operator-1.1.0\n</code></pre>"},{"location":"deploy/helm/#uninstall-the-chart","title":"Uninstall the Chart","text":"<pre><code># Uninstall the `kuberay-operator` release\nhelm uninstall kuberay-operator\n\n# The operator Pod should be removed.\nkubectl get pods\n# No resources found in default namespace.\n</code></pre>"},{"location":"deploy/helm/#working-with-argo-cd","title":"Working with Argo CD","text":"<p>If you are using Argo CD to manage the operator, you will encounter the issue which complains the CRDs too long. Same with this issue. The recommended solution is to split the operator into two Argo apps, such as:</p> <ul> <li>The first app just for installing the CRDs with <code>Replace=true</code> directly, snippet:</li> </ul> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: ray-operator-crds\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/ray-project/kuberay\n    targetRevision: v1.0.0-rc.0\n    path: helm-chart/kuberay-operator/crds\n  destination:\n    server: https://kubernetes.default.svc\n  syncPolicy:\n    syncOptions:\n    - Replace=true\n...\n</code></pre> <ul> <li>The second app that installs the Helm chart with <code>skipCrds=true</code> (new feature in Argo CD 2.3.0), snippet:</li> </ul> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: ray-operator\nspec:\n  source:\n    repoURL: https://github.com/ray-project/kuberay\n    targetRevision: v1.0.0-rc.0\n    path: helm-chart/kuberay-operator\n    helm:\n      skipCrds: true\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: ray-operator\n  syncPolicy:\n    syncOptions:\n    - CreateNamespace=true\n...\n</code></pre>"},{"location":"deploy/helm/#values","title":"Values","text":"Key Type Default Description nameOverride string <code>\"kuberay-operator\"</code> String to partially override release name. fullnameOverride string <code>\"kuberay-operator\"</code> String to fully override release name. componentOverride string <code>\"kuberay-operator\"</code> String to override component name. image.repository string <code>\"quay.io/kuberay/operator\"</code> Image repository. image.tag string <code>\"nightly\"</code> Image tag. image.pullPolicy string <code>\"IfNotPresent\"</code> Image pull policy. imagePullSecrets list <code>[]</code> Secrets with credentials to pull images from a private registry nodeSelector object <code>{}</code> Restrict to run on particular nodes. priorityClassName string <code>\"\"</code> Pod priorityClassName labels object <code>{}</code> Extra labels. annotations object <code>{}</code> Extra annotations. affinity object <code>{}</code> Pod affinity tolerations list <code>[]</code> Pod tolerations serviceAccount.create bool <code>true</code> Specifies whether a service account should be created. serviceAccount.name string <code>\"kuberay-operator\"</code> The name of the service account to use. If not set and create is true, a name is generated using the fullname template. logging.stdoutEncoder string <code>\"json\"</code> Log encoder to use for stdout (one of <code>json</code> or <code>console</code>). logging.fileEncoder string <code>\"json\"</code> Log encoder to use for file logging (one of <code>json</code> or <code>console</code>). logging.baseDir string <code>\"\"</code> Directory for kuberay-operator log file. logging.fileName string <code>\"\"</code> File name for kuberay-operator log file. logging.sizeLimit string <code>\"\"</code> EmptyDir volume size limit for kuberay-operator log file. batchScheduler.enabled bool <code>false</code> batchScheduler.name string <code>\"\"</code> configuration.enabled bool <code>false</code> Whether to enable the configuration feature. If enabled, a ConfigMap will be created and mounted to the operator. configuration.defaultContainerEnvs list <code>[]</code> Default environment variables to inject into all Ray containers in all RayCluster CRs. This allows user to set feature flags across all Ray pods. Example: defaultContainerEnvs: - name: RAY_enable_open_telemetry   value: \"true\" - name: RAY_metric_cardinality_level   value: \"recommended\" featureGates[0].name string <code>\"RayClusterStatusConditions\"</code> featureGates[0].enabled bool <code>true</code> featureGates[1].name string <code>\"RayJobDeletionPolicy\"</code> featureGates[1].enabled bool <code>false</code> featureGates[2].name string <code>\"RayMultiHostIndexing\"</code> featureGates[2].enabled bool <code>false</code> featureGates[3].name string <code>\"RayServiceIncrementalUpgrade\"</code> featureGates[3].enabled bool <code>false</code> metrics.enabled bool <code>true</code> Whether KubeRay operator should emit control plane metrics. metrics.serviceMonitor.enabled bool <code>false</code> Enable a prometheus ServiceMonitor metrics.serviceMonitor.interval string <code>\"30s\"</code> Prometheus ServiceMonitor interval metrics.serviceMonitor.honorLabels bool <code>true</code> When true, honorLabels preserves the metric\u2019s labels when they collide with the target\u2019s labels. metrics.serviceMonitor.selector object <code>{}</code> Prometheus ServiceMonitor selector metrics.serviceMonitor.namespace string <code>\"\"</code> Prometheus ServiceMonitor namespace operatorCommand string <code>\"/manager\"</code> Path to the operator binary leaderElectionEnabled bool <code>true</code> If leaderElectionEnabled is set to true, the KubeRay operator will use leader election for high availability. reconcileConcurrency int <code>1</code> The maximum number of reconcile operations that can be performed simultaneously. This setting controls the concurrency of the controller reconciliation loops. Higher values can improve throughput in clusters with many resources, but may increase resource consumption. kubeClient object <code>{\"burst\":200,\"qps\":100}</code> Kube Client configuration for QPS and burst settings. This setting controls the QPS and burst rate of the kube client when sending requests to the Kubernetes API server. If the QPS and burst values are too low, we may easily hit rate limits on the API server and slow down the controller reconciliation loops. kubeClient.qps float <code>100</code> The QPS value for the client communicating with the Kubernetes API server. Must be a float number. kubeClient.burst int <code>200</code> The maximum burst for throttling requests from this client to the Kubernetes API server. Must be a non-negative integer. rbacEnable bool <code>true</code> If rbacEnable is set to false, no RBAC resources will be created, including the Role for leader election, the Role for Pods and Services, and so on. crNamespacedRbacEnable bool <code>true</code> When crNamespacedRbacEnable is set to true, the KubeRay operator will create a Role for RayCluster preparation (e.g., Pods, Services) and a corresponding RoleBinding for each namespace listed in the \"watchNamespace\" parameter. Please note that even if crNamespacedRbacEnable is set to false, the Role and RoleBinding for leader election will still be created.  Note: (1) This variable is only effective when rbacEnable and singleNamespaceInstall are both set to true. (2) In most cases, it should be set to true, unless you are using a Kubernetes cluster managed by GitOps tools such as ArgoCD. singleNamespaceInstall bool <code>false</code> When singleNamespaceInstall is true: - Install namespaced RBAC resources such as Role and RoleBinding instead of cluster-scoped ones like ClusterRole and ClusterRoleBinding so that   the chart can be installed by users with permissions restricted to a single namespace.   (Please note that this excludes the CRDs, which can only be installed at the cluster scope.) - If \"watchNamespace\" is not set, the KubeRay operator will, by default, only listen   to resource events within its own namespace. env string <code>nil</code> Environment variables. resources object <code>{\"limits\":{\"cpu\":\"100m\",\"memory\":\"512Mi\"}}</code> Resource requests and limits for containers. livenessProbe.initialDelaySeconds int <code>10</code> livenessProbe.periodSeconds int <code>5</code> livenessProbe.failureThreshold int <code>5</code> readinessProbe.initialDelaySeconds int <code>10</code> readinessProbe.periodSeconds int <code>5</code> readinessProbe.failureThreshold int <code>5</code> podSecurityContext object <code>{}</code> Set up <code>securityContext</code> to improve Pod security. service.type string <code>\"ClusterIP\"</code> Service type. service.port int <code>8080</code> Service port."},{"location":"deploy/images/","title":"Container Images","text":"<p>Images for the various KubeRay components are published at the following locations:</p> <ol> <li>Quay.io</li> <li>DockerHub</li> </ol> <p>We recommend using Quay.io as the primary source for images as there are image-pull restrictions on DockerHub. DockerHub allows you to pull only 100 images per 6 hour window. Refer to DockerHub rate limiting for more details.</p>"},{"location":"deploy/images/#stable-versions","title":"Stable versions","text":"<p>For stable releases, use version tags (e.g. <code>quay.io/kuberay/operator:v1.1.0</code>).</p>"},{"location":"deploy/images/#master-commits","title":"Master commits","text":"<p>The first seven characters of the git SHA specify images built from specific commits (e.g. <code>quay.io/kuberay/operator:4892ac1</code>).</p>"},{"location":"deploy/images/#nightly-images","title":"Nightly images","text":"<p>The nightly tag specifies images built from the most recent master (e.g. <code>quay.io/kuberay/operator:nightly</code>).</p>"},{"location":"deploy/installation/","title":"Installation","text":"<p>Make sure your Kubernetes cluster and Kubectl are both at version at least 1.23.</p>"},{"location":"deploy/installation/#nightly-version","title":"Nightly version","text":"<pre><code>export KUBERAY_VERSION=master\n\n# Install CRD and KubeRay operator\nkubectl create -k \"github.com/ray-project/kuberay/ray-operator/config/default?ref=${KUBERAY_VERSION}&amp;timeout=90s\"\n</code></pre>"},{"location":"deploy/installation/#stable-version","title":"Stable version","text":""},{"location":"deploy/installation/#method-1-install-charts-from-helm-repository-recommended","title":"Method 1: Install charts from Helm repository (Recommended)","text":"<pre><code>helm repo add kuberay https://ray-project.github.io/kuberay-helm/\n\n# Install both CRDs and KubeRay operator\nhelm install kuberay-operator kuberay/kuberay-operator\n</code></pre>"},{"location":"deploy/installation/#method-2-kustomize","title":"Method 2: Kustomize","text":"<pre><code># Install CRD and KubeRay operator\nkubectl create -k \"github.com/ray-project/kuberay/ray-operator/config/default?ref=v1.1.0&amp;timeout=90s\"\n</code></pre>"},{"location":"development/development/","title":"KubeRay Development Guide","text":"<p>This guide provides an overview of the different components in the KubeRay project and instructions for developing and testing each component. Most developers will be concerned with the KubeRay Operator; the other components are optional.</p>"},{"location":"development/development/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>We use pre-commit to lint and format code before each commit.</p> <ol> <li>Install pre-commit</li> <li>Run <code>pre-commit install</code> to install the pre-commit hooks</li> </ol>"},{"location":"development/development/#kuberay-operator","title":"KubeRay Operator","text":"<p>The KubeRay Operator is responsible for managing Ray clusters on Kubernetes. To learn more about developing and testing the KubeRay Operator, please refer to the Operator Development Guide.</p>"},{"location":"development/development/#kuberay-apiserver","title":"KubeRay APIServer","text":"<p>The KubeRay APIServer is a central component that exposes the KubeRay API for managing Ray clusters. For more information about developing and testing the KubeRay APIServer, please refer to the APIServer Development Guide.</p>"},{"location":"development/development/#kuberay-python-client","title":"KubeRay Python client","text":"<p>The KubeRay Python client library provides APIs to handle RayCluster from your Python application. For more information about developing and testing the KubeRay Python client, please refer to the Python Client and Python API Client.</p>"},{"location":"development/development/#proto-and-openapi","title":"Proto and OpenAPI","text":"<p>KubeRay uses Protocol Buffers (protobuf) and OpenAPI specifications to define the API and data structures. For more information about developing and testing proto files and OpenAPI specifications, please refer to the Proto and OpenAPI Development Guide.</p>"},{"location":"development/development/#kuberay-kubectl-plugin-beta","title":"KubeRay Kubectl Plugin (beta)","text":"<p>A kubectl plugin that simplifies common workflows when deploying Ray on Kubernetes. If you aren't familiar with Kubernetes, this plugin simplifies running Ray on Kubernetes. For more information about developing and testing the KubeRay Kubectl Plugin, please refer to the Kubectl Plugin Development Guide.</p>"},{"location":"development/development/#deploying-documentation-locally","title":"Deploying Documentation Locally","text":"<p>To preview the KubeRay documentation locally, follow these steps:</p> <ul> <li>Make sure you have Docker installed on your machine.</li> <li>Open a terminal and navigate to the root directory of your KubeRay repository.</li> <li>Run the following command:</li> </ul> <pre><code>docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material\n</code></pre> <ul> <li>Open your web browser and navigate to http://127.0.0.1:8000/kuberay/ to view the documentation.</li> </ul> <p>If you make any changes to the documentation files, the local preview will automatically update to reflect those changes.</p>"},{"location":"development/release/","title":"Release","text":""},{"location":"development/release/#kuberay-release-process","title":"KubeRay Release Process","text":""},{"location":"development/release/#prerequisite","title":"Prerequisite","text":"<p>You need write access to the KubeRay repo to cut a release branch and create a release tag.</p>"},{"location":"development/release/#overview","title":"Overview","text":"<p>Each KubeRay minor release series (e.g., <code>v1.3.X</code>) is maintained on its own release branch. For example, the <code>v1.3.X</code> releases are cut from the <code>release-1.3</code> branch. Patch releases (e.g., <code>v1.3.1</code>) involve cherry-picking commits onto the corresponding release branch. Release candidates (e.g., <code>v1.4.0-rc.0</code>) are usually cut from the <code>master</code> branch.</p> <p>The high-level steps for a new minor or patch release are:</p> <ol> <li>For new minor releases, create a release branch (e.g., <code>release-1.4</code>) from <code>master</code> if it doesn't exist.</li> <li>Update KubeRay version references in the release branch.</li> <li>Create and push a new git tag based on the target release version (e.g., <code>v1.4.0</code>).</li> <li>Publish container images by running the <code>release-image-build</code> GitHub Actions workflow.</li> <li>Publish kubectl plugin binaries by running the <code>release-kubectl-plugin</code> GitHub Actions workflow.</li> <li>Update the kuberay-helm repository to publish the new Helm chart versions.</li> <li>Validate the release artifacts (images, charts).</li> <li>Generate the CHANGELOG for the release.</li> <li>Publish the release notes on GitHub Releases.</li> <li>Update Ray documentation to refer to the new KubeRay version.</li> </ol> <p>See the next section for more details on each step.</p>"},{"location":"development/release/#steps","title":"Steps","text":"<p>(Example commands use <code>v1.4.0</code> as the target release and <code>release-1.4</code> as the branch.)</p>"},{"location":"development/release/#step-1-create-or-prepare-the-release-branch","title":"Step 1: Create or Prepare the Release Branch","text":"<p>For a new minor release (e.g., <code>v1.4.0</code> from <code>master</code>):</p> <ol> <li> <p>Ensure your local <code>master</code> branch is up-to-date:</p> <pre><code># Ensure you are on the master branch\ngit checkout master\n# Fetch the latest changes from upstream\ngit fetch upstream\n# Reset your local master to match the upstream master\ngit reset --hard upstream/master\n# Or, if you have local changes you need to manage carefully:\n# git pull --rebase upstream master\n</code></pre> </li> <li> <p>Create the release branch (e.g., <code>release-1.4</code>) from <code>upstream/master</code>:</p> <pre><code># Create the branch locally from the latest upstream master\ngit checkout -b release-1.4 upstream/master\n</code></pre> </li> <li> <p>Push the new release branch to the <code>upstream</code> repository:</p> <pre><code>git push upstream release-1.4\n</code></pre> </li> <li> <p>Set the upstream tracking branch locally to avoid accidentally merging <code>master</code> later:</p> <pre><code>git branch --set-upstream-to upstream/release-1.4\n</code></pre> </li> <li> <p>Verify CI: GitHub Actions workflows run on <code>master</code> and all <code>release-*</code> branches. Check the Actions tab for the <code>kuberay</code> repository and ensure all workflows are passing on the newly created <code>release-1.4</code> branch.</p> </li> <li> <p>Update Ray CI to trigger nightly tests against the new kuberay release branch. See example PR #51539.</p> </li> </ol>"},{"location":"development/release/#step-2-update-kuberay-version-references","title":"Step 2: Update KubeRay Version References","text":"<p>On the release branch (<code>release-1.4</code> in this example), update all references to the KubeRay version number (e.g., <code>1.4.0</code>).</p> <p>Update the version in the following files:</p> <ul> <li><code>helm-chart/kuberay-apiserver/Chart.yaml</code> (chart version)</li> <li><code>helm-chart/kuberay-apiserver/values.yaml</code> (image tag)</li> <li><code>helm-chart/kuberay-operator/Chart.yaml</code> (chart version)</li> <li><code>helm-chart/kuberay-operator/values.yaml</code> (image tag)</li> <li><code>helm-chart/ray-cluster/Chart.yaml</code> (chart version)</li> <li><code>ray-operator/config/default/kustomization.yaml</code> (image tag)</li> <li><code>ray-operator/controllers/ray/utils/constant.go</code> (<code>KUBERAY_VERSION</code> constant)</li> </ul> <p>Open a PR to the release branch with these changes. Refer to a previous version bump PR for guidance, like PR #3071.</p>"},{"location":"development/release/#step-3-create-and-push-the-git-tag","title":"Step 3: Create and Push the Git Tag","text":"<ol> <li> <p>Ensure you are on the correct release branch and it includes the version bump commit:</p> <pre><code>git checkout release-1.4\ngit pull --rebase upstream release-1.4\n</code></pre> </li> <li> <p>Verify the latest commit is the version bump commit:</p> <pre><code>git log -1\n# Check the commit message and changes:\n# git show\n</code></pre> </li> <li> <p>Create the git tag (using the <code>vX.Y.Z</code> format, e.g., <code>v1.4.0</code>):</p> <pre><code># Create a tag\ngit tag v1.4.0\n</code></pre> </li> <li> <p>Push the tag to the <code>upstream</code> repository:</p> <pre><code>git push upstream v1.4.0\n</code></pre> </li> </ol>"},{"location":"development/release/#step-4-publish-release-images","title":"Step 4: Publish Release Images","text":"<p>Trigger the <code>release-image-build</code> workflow to build and publish the KubeRay container images.</p> <ol> <li>Navigate to the workflow page: https://github.com/ray-project/kuberay/actions/workflows/image-release.yaml</li> <li>Click the \"Run workflow\" dropdown button.</li> <li>Set the parameters:<ul> <li>Use workflow from: Select <code>Tags</code> and choose the tag you just pushed (e.g., <code>v1.4.0</code>).</li> </ul> </li> <li>Click \"Run workflow\".</li> </ol> <p>Verification: Monitor the workflow run. Once completed successfully, check that the corresponding image tags are available on quay.io.</p>"},{"location":"development/release/#step-5-publish-kubectl-plugin-release","title":"Step 5: Publish Kubectl Plugin Release","text":"<p>Trigger the <code>release-kubectl-plugin</code> workflow to build and publish the <code>kubectl-ray</code> binary.</p> <ol> <li>Navigate to the workflow page: https://github.com/ray-project/kuberay/actions/workflows/kubectl-plugin-release.yaml</li> <li>Click the \"Run workflow\" dropdown button.</li> <li>Set the parameters:<ul> <li>Use workflow from: Select <code>Tags</code> and choose the tag for the release (e.g., <code>v1.4.0</code>).</li> </ul> </li> <li>Click \"Run workflow\".</li> </ol> <p>Verification:</p> <ul> <li>Monitor the workflow run for success.</li> <li>Check the KubeRay Releases page. A draft release corresponding to the tag <code>v1.4.0</code> should have been created (or updated if it existed) with the plugin binaries attached as assets.</li> <li>The workflow should automatically open a Pull Request in the krew-index repository to update the plugin version for <code>kubectl krew</code>. Check for this PR and ensure it looks correct. It might require manual approval/merge by krew maintainers.</li> </ul>"},{"location":"development/release/#step-6-publish-kuberay-helm-charts","title":"Step 6: Publish KubeRay Helm Charts","text":"<p>Helm charts are published via the ray-project/kuberay-helm repository. This repo uses release branches (<code>release-X.Y</code>) mirroring the main <code>kuberay</code> repo. See helm-chart.md for the end-to-end workflow. Below are steps to cut a new release branch in the kuberay-helm repo and publish new charts.</p> <ol> <li> <p>Create Release Branch (if it doesn't exist):</p> <ul> <li>Clone the <code>kuberay-helm</code> repository if you haven't already.</li> <li>Set up an <code>upstream</code> remote: <code>git remote add upstream git@github.com:ray-project/kuberay-helm.git</code></li> <li> <p>If the branch <code>release-1.4</code> does not exist in <code>kuberay-helm</code>:</p> <pre><code>cd kuberay-helm\ngit fetch upstream\n# Create the branch from the latest main branch\ngit checkout -b release-1.4 upstream/main\ngit push upstream release-1.4\n# Set tracking branch\ngit branch --set-upstream-to upstream/release-1.4\ncd ..\n</code></pre> </li> <li> <p>Note: Creating a new release branch might trigger CI in <code>kuberay-helm</code>, but it shouldn't publish new chart versions yet as the chart files haven't changed.</p> </li> </ul> </li> <li> <p>Sync Helm Charts:</p> <ul> <li> <p>Ensure your local <code>kuberay</code> repository is on the release branch (<code>release-1.4</code>) and up-to-date (including the version bump from Step 2).</p> <pre><code>cd kuberay\ngit checkout release-1.4\ngit pull --rebase upstream release-1.4\ncd ..\n</code></pre> </li> <li> <p>Copy the updated Helm charts from the <code>kuberay</code> repo to the <code>kuberay-helm</code> repo. This assumes <code>kuberay</code> and <code>kuberay-helm</code> are in the same parent directory.</p> <pre><code>cd kuberay-helm\n# Ensure you are on the correct release branch\ngit checkout release-1.4\ngit pull --rebase upstream release-1.4\n\n# Remove old charts first to handle deleted files\nrm -rf helm-chart/\n# Copy the updated charts from the kuberay repo\ncp -R ../kuberay/helm-chart/ .\n</code></pre> </li> </ul> </li> <li> <p>Commit and Create Pull Request:</p> <ul> <li> <p>Stage the changes in the <code>kuberay-helm</code> repository:</p> <pre><code>git status # Verify changes\ngit add helm-chart/\ngit commit -m \"Update Helm charts to KubeRay v1.4.0\"\n</code></pre> </li> <li> <p>Push the changes to your fork of <code>kuberay-helm</code> (or directly if you have permissions, though PR is safer):</p> <pre><code># Example assuming 'origin' remote points to your fork\ngit push origin release-1.4\n</code></pre> </li> <li> <p>Open a Pull Request from your fork. See example PR #54.</p> </li> </ul> </li> <li> <p>Merge and Verify:</p> <ul> <li>Once the PR is reviewed and merged, monitor the GitHub Actions workflows in the <code>kuberay-helm</code> repository:<ul> <li><code>chart-release</code></li> <li><code>pages-build-deployment</code></li> </ul> </li> <li>Verify that both workflows succeed. This indicates the charts have been packaged and added to the Helm repository index hosted via GitHub Pages.</li> </ul> </li> </ol>"},{"location":"development/release/#step-7-validate-the-release","title":"Step 7: Validate the Release","text":"<p>Perform basic validation to ensure the released artifacts work together.</p> <ol> <li> <p>Update your local Helm repository:</p> <pre><code>helm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm repo update kuberay\n</code></pre> </li> <li> <p>Search for the new chart versions:</p> <pre><code>helm search repo kuberay/kuberay-operator --versions\nhelm search repo kuberay/ray-cluster --versions\n# Verify that version 1.4.0 (or your target version) appears\n</code></pre> </li> <li> <p>Install KubeRay using the new chart versions on a test cluster (e.g., Kind, Minikube):</p> <pre><code># Example using Kind\nkind create cluster\n\n# Install the operator using the specific version\nhelm install kuberay-operator kuberay/kuberay-operator --version &lt;new-version&gt;\n\n# Install a sample RayCluster using the specific chart version\nhelm install raycluster kuberay/ray-cluster --version &lt;new-version&gt;\n</code></pre> </li> <li> <p>Check the pods and basic functionality:</p> <pre><code>kubectl get pods\n</code></pre> <p>You should see the kuberay-operator pod in <code>Running</code> state and 1 active RayCluster.</p> </li> <li> <p>Validate Go modules are published:</p> <pre><code>go install github.com/ray-project/kuberay/ray-operator@v1.4.0\n</code></pre> </li> <li> <p>Open a PR to the master branch to update the version for KubeRay upgrade tests and make sure all tests pass. Example PR: #3825</p> </li> </ol>"},{"location":"development/release/#step-8-generate-the-changelog","title":"Step 8: Generate the CHANGELOG","text":"<p>Follow Generating the changelog for a release to generate the change log for the new release.</p>"},{"location":"development/release/#step-9-publish-release-notes","title":"Step 9: Publish Release Notes","text":"<ol> <li>Go to the KubeRay Releases page.</li> <li>Find the draft release that was created automatically in Step 5 or create a new one if needed, targeting the tag <code>v1.4.0</code>.</li> <li>Click \"Edit\" on the draft release.</li> <li>Paste the generated CHANGELOG content from Step 8 into the release description.</li> <li>Add any additional release highlights, bug fixes, known issues, etc.</li> <li>Ensure the correct tag (<code>v1.4.0</code>) is selected.</li> <li>Verify the attached assets (kubectl plugins) are correct.</li> <li>Mark the release as \"Latest release\" if appropriate (usually for the newest stable release).</li> <li>Click \"Publish release\".</li> </ol> <p>Announce the new release in the kuberay Slack channel.</p>"},{"location":"development/release/#step-10-update-rayio-documentation","title":"Step 10: Update ray.io documentation","text":"<p>Update all references to the KubeRay version in the ray.io documentation.</p>"},{"location":"guidance/rayStartParams/","title":"rayStartParams","text":""},{"location":"guidance/rayStartParams/#default-ray-start-parameters-for-kuberay","title":"Default Ray Start Parameters for KubeRay","text":"<p>This document outlines the default settings for <code>rayStartParams</code> in KubeRay.</p>"},{"location":"guidance/rayStartParams/#options-exclusive-to-the-head-pod","title":"Options Exclusive to the Head Pod","text":"<ul> <li> <p><code>--dashboard-host</code>: Host for the dashboard server, either <code>localhost</code> (127.0.0.1) or <code>0.0.0.0</code>. The latter setting exposes the Ray dashboard outside the Ray cluster, which is required when ingress is utilized for Ray cluster access. The default value for both Ray and KubeRay 0.5.0 is <code>localhost</code>. Please note that this will change for versions of KubeRay later than 0.5.0, where the default setting will be <code>0.0.0.0</code>.</p> </li> <li> <p><code>--no-monitor</code> (Modification is not recommended):</p> </li> <li>Ray autoscaler supports various node providers such as AWS, GCP, Azure, and Kubernetes. However, the default autoscaler is not compatible with Kubernetes. Therefore, when KubeRay autoscaling is enabled (i.e. <code>EnableInTreeAutoscaling</code> is true), KubeRay disables the monitor process via setting <code>--no-monitor</code> to true and injects a sidecar container for KubeRay autoscaler. See PR #13505 for more details.</li> <li> <p>Please note that the monitor process serves not only for autoscaling but also for observability, such as Prometheus metrics. Considering this, it is reasonable to disable the Kubernetes-incompatible autoscaler regardless of the value of <code>EnableInTreeAutoscaling</code>. To achieve this, we can launch the monitor process without autoscaling functionality by setting the autoscaler to READONLY mode. If <code>autoscaling-option</code> is not set, the autoscaler will default to READONLY mode.</p> </li> <li> <p><code>--port</code>: Port for the GCS server. The port is set to <code>6379</code> by default. Please ensure that this value matches the <code>gcs-server</code> container port in Ray head container.</p> </li> <li> <p><code>--redis-password</code>: Redis password for an external Redis, necessary when fault tolerance is enabled. The default value is <code>\"\"</code> after Ray 2.3.0. See #929 for more details.</p> </li> </ul>"},{"location":"guidance/rayStartParams/#options-exclusive-to-worker-pods","title":"Options Exclusive to Worker Pods","text":"<ul> <li><code>--address</code>: Address of the GCS server. Worker pods utilize this address to establish a connection with the Ray cluster. By default, this address takes the form <code>&lt;FQDN&gt;:&lt;GCS_PORT&gt;</code>. The <code>GCS_PORT</code> corresponds to the value set in the <code>--port</code> option. For more insights on Fully Qualified Domain Name (FQDN), refer to PR #938 and PR #951.</li> </ul>"},{"location":"guidance/rayStartParams/#options-applicable-to-both-head-and-worker-pods","title":"Options Applicable to Both Head and Worker Pods","text":"<ul> <li> <p><code>--block</code>: This option blocks the ray start command indefinitely. It will be automatically set by KubeRay. See PR #675 for more details. Modification is not recommended.</p> </li> <li> <p><code>--memory</code>: Amount of memory on this Ray node. Default is determined by Ray container resource limits. Modify Ray container resource limits instead of this option. See PR #170.</p> </li> <li> <p><code>--metrics-export-port</code>: Port for exposing Ray metrics through a Prometheus endpoint. The port is set to <code>8080</code> by default. Please ensure that this value matches the <code>metrics</code> container port if you need to customize it. See PR #954 and prometheus-grafana doc for more details.</p> </li> <li> <p><code>--num-cpus</code>: Number of logical CPUs on this Ray node. Default is determined by Ray container resource limits. Modify Ray container resource limits instead of this option. See PR #170. However, it is sometimes useful to override this autodetected value. For example, setting <code>num-cpus:\"0\"</code> for the Ray head pod will prevent Ray workloads with non-zero CPU requirements from being scheduled on the head.</p> </li> <li> <p><code>--num-gpus</code>: Number of GPUs on this Ray node. Default is determined by Ray container resource limits. Modify Ray container resource limits instead of this option. See PR #170.</p> </li> </ul>"},{"location":"guidance/rayclient-nginx-ingress/","title":"Rayclient nginx ingress","text":""},{"location":"guidance/rayclient-nginx-ingress/#connect-to-ray-client-with-nginx-ingress","title":"Connect to Ray client with NGINX Ingress","text":"<p>Warning: Ray client has some known limitations and is not actively maintained.</p> <p>This document provides an example for connecting Ray client to a Raycluster via NGINX Ingress on Kind. Although this is a Kind example, the steps apply to any Kubernetes Cluster that runs the NGINX Ingress Controller.</p>"},{"location":"guidance/rayclient-nginx-ingress/#requirements","title":"Requirements","text":"<ul> <li>Environment:</li> <li><code>Ubuntu</code></li> <li> <p><code>Kind</code></p> </li> <li> <p>Computing resources:</p> </li> <li>16GB RAM</li> <li>8 CPUs</li> </ul>"},{"location":"guidance/rayclient-nginx-ingress/#step-1-create-a-kind-cluster","title":"Step 1: Create a Kind cluster","text":"<p>The extra arg prepares the Kind cluster for deploying the ingress controller</p> <pre><code>cat &lt;&lt;EOF | kind create cluster --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 80\n    protocol: TCP\n  - containerPort: 443\n    hostPort: 443\n    protocol: TCP\nEOF\n</code></pre>"},{"location":"guidance/rayclient-nginx-ingress/#step-2-deploy-nginx-ingress-controller","title":"Step 2: Deploy NGINX Ingress Controller","text":"<p>The SSL Passthrough feature is required to pass on the encryption to the backend service directly.</p> <pre><code># Deploy the NGINX Ingress Controller\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml\n\n# Turn on SSL Passthrough\nkubectl patch deploy --type json --patch '[{\"op\":\"add\",\"path\": \"/spec/template/spec/containers/0/args/-\",\"value\":\"--enable-ssl-passthrough\"}]' ingress-nginx-controller -n ingress-nginx\n\n# Verify log has Starting TLS proxy for SSL Passthrough\nkubectl logs deploy/ingress-nginx-controller -n ingress-nginx\n</code></pre>"},{"location":"guidance/rayclient-nginx-ingress/#step-3-install-kuberay-operator","title":"Step 3: Install KubeRay operator","text":"<p>Follow this document to install the latest stable KubeRay operator via Helm repository.</p>"},{"location":"guidance/rayclient-nginx-ingress/#step-4-create-a-raycluster-with-tls-enabled","title":"Step 4: Create a Raycluster with TLS enabled","text":"<p>The Ray client server is a GRPC service. The NGINX Ingress Controller supports GRPC backend service which uses http/2 and requires secured connection. The command below creates a Raycluster with TLS enabled:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-cluster.tls.yaml\n</code></pre> <p>Refer to the TLS document for more detail.</p>"},{"location":"guidance/rayclient-nginx-ingress/#step-5-create-an-ingress-for-the-ray-client-service","title":"Step 5: Create an ingress for the Ray client service","text":"<p>With the Raycluster running, create an ingress for the Ray client backend service using the rayclient-ingress example below:</p> <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: rayclient-ingress\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"\n    nginx.ingress.kubernetes.io/backend-protocol: \"GRPC\"\nspec:\n  rules:\n    - host: \"localhost\"\n      http:\n        paths:\n        - path: /\n          pathType: Prefix\n          backend:\n            service:\n              name: raycluster-tls-head-svc\n              port:\n                number: 10001\nEOF\n</code></pre> <p>The annotation, <code>nginx.ingress.kubernetes.io/backend-protocol: \"GRPC\"</code> sets up the appropriate NGINX configuration to route http/2 traffic to a GRPC backend service. The <code>nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"</code> annotation tells the ingress to forward the encrypted traffic to the backend service to be handled inside the Raycluster itself.</p>"},{"location":"guidance/rayclient-nginx-ingress/#step-6-connecting-to-ray-client-service-via-the-ingress","title":"Step 6: Connecting to Ray client service via the ingress","text":"<p>Since the Raycluster uses TLS, the local Ray client would require a set of certificates to connect to Raycluster.</p> <p>Warning: Ray client has some known limitations and is not actively maintained.</p> <pre><code># Download the ca key pair and create a cert signing request (CSR)\nkubectl get secret ca-tls -o template='{{index .data \"ca.key\"}}'|base64 -d &gt; ./ca.key\nkubectl get secret ca-tls -o template='{{index .data \"ca.crt\"}}'|base64 -d &gt; ./ca.crt\nopenssl req -nodes -newkey rsa:2048 -keyout ./tls.key -out ./tls.csr -subj '/CN=local'\ncat &lt;&lt;EOF &gt;./cert.conf\nauthorityKeyIdentifier=keyid,issuer\nbasicConstraints=CA:FALSE\nsubjectAltName = @alt_names\n[alt_names]\nDNS.1 = localhost\nIP.1 = 127.0.0.1\nEOF\n# Sign and create a tls cert\nopenssl x509 -req -CA ./ca.crt -CAkey ./ca.key -in ./tls.csr -out ./tls.crt -days 365 -CAcreateserial -extfile ./cert.conf\n\n# Connect Ray client to the Raycluster using the tls keypair and the ca cert\npython -c '\nimport os\nimport ray\nos.environ[\"RAY_USE_TLS\"] = \"1\"\nos.environ[\"RAY_TLS_SERVER_CERT\"] = os.path.join(\"./\", \"tls.crt\")\nos.environ[\"RAY_TLS_SERVER_KEY\"] = os.path.join(\"./\", \"tls.key\")\nos.environ[\"RAY_TLS_CA_CERT\"] = os.path.join(\"./\", \"ca.crt\")\nray.init(address=\"ray://localhost\", logging_level=\"DEBUG\")'\n</code></pre> <p>The output should be similar to:</p> <pre><code>2023-04-25 16:33:32,452   INFO client_builder.py:253 -- Passing the following kwargs to ray.init() on the server: logging_level\n2023-04-25 16:33:32,460   DEBUG worker.py:378 -- client gRPC channel state change: ChannelConnectivity.IDLE\n2023-04-25 16:33:32,664   DEBUG worker.py:378 -- client gRPC channel state change: ChannelConnectivity.CONNECTING\n2023-04-25 16:33:32,671   DEBUG worker.py:378 -- client gRPC channel state change: ChannelConnectivity.READY\n</code></pre>"},{"location":"reference/api/","title":"API Reference","text":""},{"location":"reference/api/#packages","title":"Packages","text":"<ul> <li>ray.io/v1</li> <li>ray.io/v1alpha1</li> </ul>"},{"location":"reference/api/#rayiov1","title":"ray.io/v1","text":"<p>Package v1 contains API Schema definitions for the ray v1 API group</p>"},{"location":"reference/api/#resource-types","title":"Resource Types","text":"<ul> <li>RayCluster</li> <li>RayJob</li> <li>RayService</li> </ul>"},{"location":"reference/api/#autoscaleroptions","title":"AutoscalerOptions","text":"<p>AutoscalerOptions specifies optional configuration for the Ray autoscaler.</p> <p>Appears in: - RayClusterSpec</p> Field Description Default Validation <code>resources</code> ResourceRequirements Resources specifies optional resource request and limit overrides for the autoscaler container.Default values: 500m CPU request and limit. 512Mi memory request and limit. <code>image</code> string Image optionally overrides the autoscaler's container image. This override is provided for autoscaler testing and development. <code>imagePullPolicy</code> PullPolicy ImagePullPolicy optionally overrides the autoscaler container's image pull policy. This override is provided for autoscaler testing and development. <code>securityContext</code> SecurityContext SecurityContext defines the security options the container should be run with.If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ <code>idleTimeoutSeconds</code> integer IdleTimeoutSeconds is the number of seconds to wait before scaling down a worker pod which is not using Ray resources.Defaults to 60 (one minute). It is not read by the KubeRay operator but by the Ray autoscaler. <code>upscalingMode</code> UpscalingMode UpscalingMode is \"Conservative\", \"Default\", or \"Aggressive.\"Conservative: Upscaling is rate-limited; the number of pending worker pods is at most the size of the Ray cluster.Default: Upscaling is not rate-limited.Aggressive: An alias for Default; upscaling is not rate-limited.It is not read by the KubeRay operator but by the Ray autoscaler. Enum: [Default Aggressive Conservative]  <code>version</code> AutoscalerVersion Version is the version of the Ray autoscaler.Setting this to v1 will explicitly use autoscaler v1.Setting this to v2 will explicitly use autoscaler v2.If this isn't set, the Ray version determines the autoscaler version.In Ray 2.47.0 and later, the default autoscaler version is v2. It's v1 before that. Enum: [v1 v2]  <code>env</code> EnvVar array Optional list of environment variables to set in the autoscaler container. <code>envFrom</code> EnvFromSource array Optional list of sources to populate environment variables in the autoscaler container. <code>volumeMounts</code> VolumeMount array Optional list of volumeMounts.  This is needed for enabling TLS for the autoscaler container."},{"location":"reference/api/#autoscalerversion","title":"AutoscalerVersion","text":"<p>Underlying type: string</p> <p>Validation: - Enum: [v1 v2]</p> <p>Appears in: - AutoscalerOptions</p>"},{"location":"reference/api/#clusterupgradeoptions","title":"ClusterUpgradeOptions","text":"<p>These options are currently only supported for the IncrementalUpgrade type.</p> <p>Appears in: - RayServiceUpgradeStrategy</p> Field Description Default Validation <code>maxSurgePercent</code> integer The capacity of serve requests the upgraded cluster should scale to handle each interval.Defaults to 100%. 100 <code>stepSizePercent</code> integer The percentage of traffic to switch to the upgraded RayCluster at a set interval after scaling by MaxSurgePercent. <code>intervalSeconds</code> integer The interval in seconds between transferring StepSize traffic from the old to new RayCluster. <code>gatewayClassName</code> string The name of the Gateway Class installed by the Kubernetes Cluster admin."},{"location":"reference/api/#deletioncondition","title":"DeletionCondition","text":"<p>DeletionCondition specifies the trigger conditions for a deletion action.</p> <p>Appears in: - DeletionRule</p> Field Description Default Validation <code>ttlSeconds</code> integer TTLSeconds is the time in seconds from when the JobStatusreaches the specified terminal state to when this deletion action should be triggered.The value must be a non-negative integer. 0 Minimum: 0"},{"location":"reference/api/#deletionpolicy","title":"DeletionPolicy","text":"<p>DeletionPolicy is the legacy single-stage deletion policy. Deprecated: This struct is part of the legacy API. Use DeletionRule for new configurations.</p> <p>Appears in: - DeletionStrategy</p> Field Description Default Validation <code>policy</code> DeletionPolicyType Policy is the action to take when the condition is met.This field is logically required when using the legacy OnSuccess/OnFailure policies.It is marked as '+optional' at the API level to allow the 'deletionRules' field to be used instead. Enum: [DeleteCluster DeleteWorkers DeleteSelf DeleteNone]"},{"location":"reference/api/#deletionpolicytype","title":"DeletionPolicyType","text":"<p>Underlying type: string</p> <p>Appears in: - DeletionPolicy - DeletionRule</p>"},{"location":"reference/api/#deletionrule","title":"DeletionRule","text":"<p>DeletionRule defines a single deletion action and its trigger condition. This is the new, recommended way to define deletion behavior.</p> <p>Appears in: - DeletionStrategy</p> Field Description Default Validation <code>policy</code> DeletionPolicyType Policy is the action to take when the condition is met. This field is required. Enum: [DeleteCluster DeleteWorkers DeleteSelf DeleteNone]  <code>condition</code> DeletionCondition The condition under which this deletion rule is triggered. This field is required."},{"location":"reference/api/#deletionstrategy","title":"DeletionStrategy","text":"<p>DeletionStrategy configures automated cleanup after the RayJob reaches a terminal state. Two mutually exclusive styles are supported:</p> <pre><code>Legacy: provide both onSuccess and onFailure (deprecated; removal planned for 1.6.0). May be combined with shutdownAfterJobFinishes and (optionally) global TTLSecondsAfterFinished.\nRules: provide deletionRules (non-empty list). Rules mode is incompatible with shutdownAfterJobFinishes, legacy fields, and the global TTLSecondsAfterFinished (use per\u2011rule condition.ttlSeconds instead).\n</code></pre> <p>Semantics:   - A non-empty deletionRules selects rules mode; empty lists are treated as unset.   - Legacy requires both onSuccess and onFailure; specifying only one is invalid.   - Global TTLSecondsAfterFinished &gt; 0 requires shutdownAfterJobFinishes=true; therefore it cannot be used with rules mode or with legacy alone (no shutdown).   - Feature gate RayJobDeletionPolicy must be enabled when this block is present.</p> <p>Validation:   - CRD XValidations prevent mixing legacy fields with deletionRules and enforce legacy completeness.   - Controller logic enforces rules vs shutdown exclusivity and TTL constraints.   - onSuccess/onFailure are deprecated; migration to deletionRules is encouraged.</p> <p>Appears in: - RayJobSpec</p> Field Description Default Validation <code>onSuccess</code> DeletionPolicy OnSuccess is the deletion policy for a successful RayJob.Deprecated: Use <code>deletionRules</code> instead for more flexible, multi-stage deletion strategies.This field will be removed in release 1.6.0. <code>onFailure</code> DeletionPolicy OnFailure is the deletion policy for a failed RayJob.Deprecated: Use <code>deletionRules</code> instead for more flexible, multi-stage deletion strategies.This field will be removed in release 1.6.0. <code>deletionRules</code> DeletionRule array DeletionRules is a list of deletion rules, processed based on their trigger conditions.While the rules can be used to define a sequence, if multiple rules are overdue (e.g., due to controller downtime),the most impactful rule (e.g., DeleteSelf) will be executed first to prioritize resource cleanup. MinItems: 1"},{"location":"reference/api/#gcsfaulttoleranceoptions","title":"GcsFaultToleranceOptions","text":"<p>GcsFaultToleranceOptions contains configs for GCS FT</p> <p>Appears in: - RayClusterSpec</p> Field Description Default Validation <code>redisUsername</code> RedisCredential <code>redisPassword</code> RedisCredential <code>externalStorageNamespace</code> string <code>redisAddress</code> string"},{"location":"reference/api/#headgroupspec","title":"HeadGroupSpec","text":"<p>HeadGroupSpec are the spec for the head pod</p> <p>Appears in: - RayClusterSpec</p> Field Description Default Validation <code>template</code> PodTemplateSpec Template is the exact pod template used in K8s deployments, statefulsets, etc. <code>headService</code> Service HeadService is the Kubernetes service of the head pod. <code>enableIngress</code> boolean EnableIngress indicates whether operator should create ingress object for head service or not. <code>resources</code> object (keys:string, values:string) Resources specifies the resource quantities for the head group.These values override the resources passed to <code>rayStartParams</code> for the group, buthave no effect on the resources set at the K8s Pod container level. <code>labels</code> object (keys:string, values:string) Labels specifies the Ray node labels for the head group.These labels will also be added to the Pods of this head group and override the <code>--labels</code>argument passed to <code>rayStartParams</code>. <code>rayStartParams</code> object (keys:string, values:string) RayStartParams are the params of the start command: node-manager-port, object-store-memory, ... <code>serviceType</code> ServiceType ServiceType is Kubernetes service type of the head service. it will be used by the workers to connect to the head pod"},{"location":"reference/api/#jobsubmissionmode","title":"JobSubmissionMode","text":"<p>Underlying type: string</p> <p>Appears in: - RayJobSpec</p>"},{"location":"reference/api/#raycluster","title":"RayCluster","text":"<p>RayCluster is the Schema for the RayClusters API</p> Field Description Default Validation <code>apiVersion</code> string <code>ray.io/v1</code> <code>kind</code> string <code>RayCluster</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> RayClusterSpec Specification of the desired behavior of the RayCluster."},{"location":"reference/api/#rayclusterspec","title":"RayClusterSpec","text":"<p>RayClusterSpec defines the desired state of RayCluster</p> <p>Appears in: - RayCluster - RayJobSpec - RayServiceSpec</p> Field Description Default Validation <code>suspend</code> boolean Suspend indicates whether a RayCluster should be suspended.A suspended RayCluster will have head pods and worker pods deleted. <code>managedBy</code> string ManagedBy is an optional configuration for the controller or entity that manages a RayCluster.The value must be either 'ray.io/kuberay-operator' or 'kueue.x-k8s.io/multikueue'.The kuberay-operator reconciles a RayCluster which doesn't have this field at all orthe field value is the reserved string 'ray.io/kuberay-operator',but delegates reconciling the RayCluster with 'kueue.x-k8s.io/multikueue' to the Kueue.The field is immutable. <code>autoscalerOptions</code> AutoscalerOptions AutoscalerOptions specifies optional configuration for the Ray autoscaler. <code>headServiceAnnotations</code> object (keys:string, values:string) <code>enableInTreeAutoscaling</code> boolean EnableInTreeAutoscaling indicates whether operator should create in tree autoscaling configs <code>gcsFaultToleranceOptions</code> GcsFaultToleranceOptions GcsFaultToleranceOptions for enabling GCS FT <code>headGroupSpec</code> HeadGroupSpec HeadGroupSpec is the spec for the head pod <code>rayVersion</code> string RayVersion is used to determine the command for the Kubernetes Job managed by RayJob <code>workerGroupSpecs</code> WorkerGroupSpec array WorkerGroupSpecs are the specs for the worker pods"},{"location":"reference/api/#rayjob","title":"RayJob","text":"<p>RayJob is the Schema for the rayjobs API</p> Field Description Default Validation <code>apiVersion</code> string <code>ray.io/v1</code> <code>kind</code> string <code>RayJob</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> RayJobSpec"},{"location":"reference/api/#rayjobspec","title":"RayJobSpec","text":"<p>RayJobSpec defines the desired state of RayJob</p> <p>Appears in: - RayJob</p> Field Description Default Validation <code>activeDeadlineSeconds</code> integer ActiveDeadlineSeconds is the duration in seconds that the RayJob may be active beforeKubeRay actively tries to terminate the RayJob; value must be positive integer. <code>backoffLimit</code> integer Specifies the number of retries before marking this job failed.Each retry creates a new RayCluster. 0 <code>rayClusterSpec</code> RayClusterSpec RayClusterSpec is the cluster template to run the job <code>submitterPodTemplate</code> PodTemplateSpec SubmitterPodTemplate is the template for the pod that will run <code>ray job submit</code>. <code>metadata</code> object (keys:string, values:string) Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>clusterSelector</code> object (keys:string, values:string) clusterSelector is used to select running rayclusters by labels <code>submitterConfig</code> SubmitterConfig Configurations of submitter k8s job. <code>managedBy</code> string ManagedBy is an optional configuration for the controller or entity that manages a RayJob.The value must be either 'ray.io/kuberay-operator' or 'kueue.x-k8s.io/multikueue'.The kuberay-operator reconciles a RayJob which doesn't have this field at all orthe field value is the reserved string 'ray.io/kuberay-operator',but delegates reconciling the RayJob with 'kueue.x-k8s.io/multikueue' to the Kueue.The field is immutable. <code>deletionStrategy</code> DeletionStrategy DeletionStrategy automates post-completion cleanup.Choose one style or omit:  - Legacy: both onSuccess &amp; onFailure (deprecated; may combine with shutdownAfterJobFinishes and TTLSecondsAfterFinished).  - Rules: deletionRules (non-empty) \u2014 incompatible with shutdownAfterJobFinishes, legacy fields, and global TTLSecondsAfterFinished (use per-rule condition.ttlSeconds).Global TTLSecondsAfterFinished &gt; 0 requires shutdownAfterJobFinishes=true.Feature gate RayJobDeletionPolicy must be enabled when this field is set. <code>entrypoint</code> string Entrypoint represents the command to start execution. <code>runtimeEnvYAML</code> string RuntimeEnvYAML represents the runtime environment configurationprovided as a multi-line YAML string. <code>jobId</code> string If jobId is not set, a new jobId will be auto-generated. <code>submissionMode</code> JobSubmissionMode SubmissionMode specifies how RayJob submits the Ray job to the RayCluster.In \"K8sJobMode\", the KubeRay operator creates a submitter Kubernetes Job to submit the Ray job.In \"HTTPMode\", the KubeRay operator sends a request to the RayCluster to create a Ray job.In \"InteractiveMode\", the KubeRay operator waits for a user to submit a job to the Ray cluster.In \"SidecarMode\", the KubeRay operator injects a container into the Ray head Pod that acts as the job submitter to submit the Ray job. K8sJobMode <code>entrypointResources</code> string EntrypointResources specifies the custom resources and quantities to reserve for theentrypoint command. <code>entrypointNumCpus</code> float EntrypointNumCpus specifies the number of cpus to reserve for the entrypoint command. <code>entrypointNumGpus</code> float EntrypointNumGpus specifies the number of gpus to reserve for the entrypoint command. <code>ttlSecondsAfterFinished</code> integer TTLSecondsAfterFinished is the TTL to clean up RayCluster.It's only working when ShutdownAfterJobFinishes set to true. 0 <code>shutdownAfterJobFinishes</code> boolean ShutdownAfterJobFinishes will determine whether to delete the ray cluster once rayJob succeed or failed. <code>suspend</code> boolean suspend specifies whether the RayJob controller should create a RayCluster instanceIf a job is applied with the suspend field set to true,the RayCluster will not be created and will wait for the transition to false.If the RayCluster is already created, it will be deleted.In case of transition to false a new RayCluster will be created."},{"location":"reference/api/#rayservice","title":"RayService","text":"<p>RayService is the Schema for the rayservices API</p> Field Description Default Validation <code>apiVersion</code> string <code>ray.io/v1</code> <code>kind</code> string <code>RayService</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> RayServiceSpec"},{"location":"reference/api/#rayservicespec","title":"RayServiceSpec","text":"<p>RayServiceSpec defines the desired state of RayService</p> <p>Appears in: - RayService</p> Field Description Default Validation <code>rayClusterDeletionDelaySeconds</code> integer RayClusterDeletionDelaySeconds specifies the delay, in seconds, before deleting old RayClusters.The default value is 60 seconds. Minimum: 0  <code>serviceUnhealthySecondThreshold</code> integer Deprecated: This field is not used anymore. ref: https://github.com/ray-project/kuberay/issues/1685 <code>deploymentUnhealthySecondThreshold</code> integer Deprecated: This field is not used anymore. ref: https://github.com/ray-project/kuberay/issues/1685 <code>serveService</code> Service ServeService is the Kubernetes service for head node and worker nodes who have healthy http proxy to serve traffics. <code>upgradeStrategy</code> RayServiceUpgradeStrategy UpgradeStrategy defines the scaling policy used when upgrading the RayService. <code>serveConfigV2</code> string Important: Run \"make\" to regenerate code after modifying this fileDefines the applications and deployments to deploy, should be a YAML multi-line scalar string. <code>rayClusterConfig</code> RayClusterSpec <code>excludeHeadPodFromServeSvc</code> boolean If the field is set to true, the value of the label <code>ray.io/serve</code> on the head Pod should always be false.Therefore, the head Pod's endpoint will not be added to the Kubernetes Serve service."},{"location":"reference/api/#rayserviceupgradestrategy","title":"RayServiceUpgradeStrategy","text":"<p>Appears in: - RayServiceSpec</p> Field Description Default Validation <code>type</code> RayServiceUpgradeType Type represents the strategy used when upgrading the RayService. Currently supports <code>NewCluster</code> and <code>None</code>. <code>clusterUpgradeOptions</code> ClusterUpgradeOptions ClusterUpgradeOptions defines the behavior of a NewClusterWithIncrementalUpgrade type.RayServiceIncrementalUpgrade feature gate must be enabled to set ClusterUpgradeOptions."},{"location":"reference/api/#rayserviceupgradetype","title":"RayServiceUpgradeType","text":"<p>Underlying type: string</p> <p>Appears in: - RayServiceUpgradeStrategy</p>"},{"location":"reference/api/#rediscredential","title":"RedisCredential","text":"<p>RedisCredential is the redis username/password or a reference to the source containing the username/password</p> <p>Appears in: - GcsFaultToleranceOptions</p> Field Description Default Validation <code>valueFrom</code> EnvVarSource <code>value</code> string"},{"location":"reference/api/#scalestrategy","title":"ScaleStrategy","text":"<p>ScaleStrategy to remove workers</p> <p>Appears in: - WorkerGroupSpec</p> Field Description Default Validation <code>workersToDelete</code> string array WorkersToDelete workers to be deleted"},{"location":"reference/api/#submitterconfig","title":"SubmitterConfig","text":"<p>Appears in: - RayJobSpec</p> Field Description Default Validation <code>backoffLimit</code> integer BackoffLimit of the submitter k8s job."},{"location":"reference/api/#upscalingmode","title":"UpscalingMode","text":"<p>Underlying type: string</p> <p>Validation: - Enum: [Default Aggressive Conservative]</p> <p>Appears in: - AutoscalerOptions</p>"},{"location":"reference/api/#workergroupspec","title":"WorkerGroupSpec","text":"<p>WorkerGroupSpec are the specs for the worker pods</p> <p>Appears in: - RayClusterSpec</p> Field Description Default Validation <code>suspend</code> boolean Suspend indicates whether a worker group should be suspended.A suspended worker group will have all pods deleted.This is not a user-facing API and is only used by RayJob DeletionStrategy. <code>groupName</code> string we can have multiple worker groups, we distinguish them by name <code>replicas</code> integer Replicas is the number of desired Pods for this worker group. See https://github.com/ray-project/kuberay/pull/1443 for more details about the reason for making this field optional. 0 <code>minReplicas</code> integer MinReplicas denotes the minimum number of desired Pods for this worker group. 0 <code>maxReplicas</code> integer MaxReplicas denotes the maximum number of desired Pods for this worker group, and the default value is maxInt32. 2147483647 <code>idleTimeoutSeconds</code> integer IdleTimeoutSeconds denotes the number of seconds to wait before the v2 autoscaler terminates an idle worker pod of this type.This value is only used with the Ray Autoscaler enabled and defaults to the value set by the AutoscalingConfig if not specified for this worker group. <code>resources</code> object (keys:string, values:string) Resources specifies the resource quantities for this worker group.These values override the resources passed to <code>rayStartParams</code> for the group, buthave no effect on the resources set at the K8s Pod container level. <code>labels</code> object (keys:string, values:string) Labels specifies the Ray node labels for this worker group.These labels will also be added to the Pods of this worker group and override the <code>--labels</code>argument passed to <code>rayStartParams</code>. <code>rayStartParams</code> object (keys:string, values:string) RayStartParams are the params of the start command: address, object-store-memory, ... <code>template</code> PodTemplateSpec Template is a pod template for the worker <code>scaleStrategy</code> ScaleStrategy ScaleStrategy defines which pods to remove <code>numOfHosts</code> integer NumOfHosts denotes the number of hosts to create per replica. The default value is 1. 1"},{"location":"reference/api/#rayiov1alpha1","title":"ray.io/v1alpha1","text":"<p>Package v1alpha1 contains API Schema definitions for the ray v1alpha1 API group</p>"},{"location":"reference/api/#resource-types_1","title":"Resource Types","text":"<ul> <li>RayCluster</li> <li>RayJob</li> <li>RayService</li> </ul>"},{"location":"reference/api/#autoscaleroptions_1","title":"AutoscalerOptions","text":"<p>AutoscalerOptions specifies optional configuration for the Ray autoscaler.</p> <p>Appears in: - RayClusterSpec</p> Field Description Default Validation <code>resources</code> ResourceRequirements Resources specifies optional resource request and limit overrides for the autoscaler container.Default values: 500m CPU request and limit. 512Mi memory request and limit. <code>image</code> string Image optionally overrides the autoscaler's container image. This override is provided for autoscaler testing and development. <code>imagePullPolicy</code> PullPolicy ImagePullPolicy optionally overrides the autoscaler container's image pull policy. This override is provided for autoscaler testing and development. <code>securityContext</code> SecurityContext SecurityContext defines the security options the container should be run with.If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ <code>idleTimeoutSeconds</code> integer IdleTimeoutSeconds is the number of seconds to wait before scaling down a worker pod which is not using Ray resources.Defaults to 60 (one minute). It is not read by the KubeRay operator but by the Ray autoscaler. <code>upscalingMode</code> UpscalingMode UpscalingMode is \"Conservative\", \"Default\", or \"Aggressive.\"Conservative: Upscaling is rate-limited; the number of pending worker pods is at most the size of the Ray cluster.Default: Upscaling is not rate-limited.Aggressive: An alias for Default; upscaling is not rate-limited.It is not read by the KubeRay operator but by the Ray autoscaler. Enum: [Default Aggressive Conservative]  <code>env</code> EnvVar array Optional list of environment variables to set in the autoscaler container. <code>envFrom</code> EnvFromSource array Optional list of sources to populate environment variables in the autoscaler container. <code>volumeMounts</code> VolumeMount array Optional list of volumeMounts.  This is needed for enabling TLS for the autoscaler container."},{"location":"reference/api/#headgroupspec_1","title":"HeadGroupSpec","text":"<p>HeadGroupSpec are the spec for the head pod</p> <p>Appears in: - RayClusterSpec</p> Field Description Default Validation <code>template</code> PodTemplateSpec Template is the exact pod template used in K8s deployments, statefulsets, etc. <code>headService</code> Service HeadService is the Kubernetes service of the head pod. <code>enableIngress</code> boolean EnableIngress indicates whether operator should create ingress object for head service or not. <code>rayStartParams</code> object (keys:string, values:string) RayStartParams are the params of the start command: node-manager-port, object-store-memory, ... <code>serviceType</code> ServiceType ServiceType is Kubernetes service type of the head service. it will be used by the workers to connect to the head pod"},{"location":"reference/api/#raycluster_1","title":"RayCluster","text":"<p>RayCluster is the Schema for the RayClusters API</p> Field Description Default Validation <code>apiVersion</code> string <code>ray.io/v1alpha1</code> <code>kind</code> string <code>RayCluster</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> RayClusterSpec Specification of the desired behavior of the RayCluster."},{"location":"reference/api/#rayclusterspec_1","title":"RayClusterSpec","text":"<p>RayClusterSpec defines the desired state of RayCluster</p> <p>Appears in: - RayCluster - RayJobSpec - RayServiceSpec</p> Field Description Default Validation <code>enableInTreeAutoscaling</code> boolean EnableInTreeAutoscaling indicates whether operator should create in tree autoscaling configs <code>autoscalerOptions</code> AutoscalerOptions AutoscalerOptions specifies optional configuration for the Ray autoscaler. <code>suspend</code> boolean Suspend indicates whether a RayCluster should be suspended.A suspended RayCluster will have head pods and worker pods deleted. <code>headServiceAnnotations</code> object (keys:string, values:string) <code>headGroupSpec</code> HeadGroupSpec HeadGroupSpec is the spec for the head pod <code>rayVersion</code> string RayVersion is used to determine the command for the Kubernetes Job managed by RayJob <code>workerGroupSpecs</code> WorkerGroupSpec array WorkerGroupSpecs are the specs for the worker pods"},{"location":"reference/api/#rayjob_1","title":"RayJob","text":"<p>RayJob is the Schema for the rayjobs API</p> Field Description Default Validation <code>apiVersion</code> string <code>ray.io/v1alpha1</code> <code>kind</code> string <code>RayJob</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> RayJobSpec"},{"location":"reference/api/#rayjobspec_1","title":"RayJobSpec","text":"<p>RayJobSpec defines the desired state of RayJob</p> <p>Appears in: - RayJob</p> Field Description Default Validation <code>submitterPodTemplate</code> PodTemplateSpec SubmitterPodTemplate is the template for the pod that will run <code>ray job submit</code>. <code>metadata</code> object (keys:string, values:string) Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>rayClusterSpec</code> RayClusterSpec RayClusterSpec is the cluster template to run the job <code>clusterSelector</code> object (keys:string, values:string) ClusterSelector is used to select running rayclusters by labels <code>entrypoint</code> string Entrypoint represents the command to start execution. <code>runtimeEnvYAML</code> string RuntimeEnvYAML represents the runtime environment configurationprovided as a multi-line YAML string. <code>jobId</code> string If jobId is not set, a new jobId will be auto-generated. <code>entrypointResources</code> string EntrypointResources specifies the custom resources and quantities to reserve for theentrypoint command. <code>ttlSecondsAfterFinished</code> integer TTLSecondsAfterFinished is the TTL to clean up RayCluster.It's only working when ShutdownAfterJobFinishes set to true. 0 <code>entrypointNumCpus</code> float EntrypointNumCpus specifies the number of cpus to reserve for the entrypoint command. <code>entrypointNumGpus</code> float EntrypointNumGpus specifies the number of gpus to reserve for the entrypoint command. <code>shutdownAfterJobFinishes</code> boolean ShutdownAfterJobFinishes will determine whether to delete the ray cluster once rayJob succeed or failed. <code>suspend</code> boolean Suspend specifies whether the RayJob controller should create a RayCluster instanceIf a job is applied with the suspend field set to true,the RayCluster will not be created and will wait for the transition to false.If the RayCluster is already created, it will be deleted.In case of transition to false a new RayCluster will be created."},{"location":"reference/api/#rayservice_1","title":"RayService","text":"<p>RayService is the Schema for the rayservices API</p> Field Description Default Validation <code>apiVersion</code> string <code>ray.io/v1alpha1</code> <code>kind</code> string <code>RayService</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> RayServiceSpec"},{"location":"reference/api/#rayservicespec_1","title":"RayServiceSpec","text":"<p>RayServiceSpec defines the desired state of RayService</p> <p>Appears in: - RayService</p> Field Description Default Validation <code>serveService</code> Service ServeService is the Kubernetes service for head node and worker nodes who have healthy http proxy to serve traffics. <code>serviceUnhealthySecondThreshold</code> integer Deprecated: This field is not used anymore. ref: https://github.com/ray-project/kuberay/issues/1685 <code>deploymentUnhealthySecondThreshold</code> integer Deprecated: This field is not used anymore. ref: https://github.com/ray-project/kuberay/issues/1685 <code>serveConfigV2</code> string Important: Run \"make\" to regenerate code after modifying this fileDefines the applications and deployments to deploy, should be a YAML multi-line scalar string. <code>rayClusterConfig</code> RayClusterSpec"},{"location":"reference/api/#scalestrategy_1","title":"ScaleStrategy","text":"<p>ScaleStrategy to remove workers</p> <p>Appears in: - WorkerGroupSpec</p> Field Description Default Validation <code>workersToDelete</code> string array WorkersToDelete workers to be deleted"},{"location":"reference/api/#upscalingmode_1","title":"UpscalingMode","text":"<p>Underlying type: string</p> <p>Validation: - Enum: [Default Aggressive Conservative]</p> <p>Appears in: - AutoscalerOptions</p>"},{"location":"reference/api/#workergroupspec_1","title":"WorkerGroupSpec","text":"<p>WorkerGroupSpec are the specs for the worker pods</p> <p>Appears in: - RayClusterSpec</p> Field Description Default Validation <code>groupName</code> string we can have multiple worker groups, we distinguish them by name <code>replicas</code> integer Replicas is the number of desired Pods for this worker group. See https://github.com/ray-project/kuberay/pull/1443 for more details about the reason for making this field optional. 0 <code>minReplicas</code> integer MinReplicas denotes the minimum number of desired Pods for this worker group. 0 <code>maxReplicas</code> integer MaxReplicas denotes the maximum number of desired Pods for this worker group, and the default value is maxInt32. 2147483647 <code>rayStartParams</code> object (keys:string, values:string) RayStartParams are the params of the start command: address, object-store-memory, ... <code>template</code> PodTemplateSpec Template is a pod template for the worker <code>scaleStrategy</code> ScaleStrategy ScaleStrategy defines which pods to remove"},{"location":"release/changelog/","title":"Changelog","text":""},{"location":"release/changelog/#generate-the-changelog-for-a-release","title":"Generate the changelog for a release","text":""},{"location":"release/changelog/#prerequisite","title":"Prerequisite","text":"<ol> <li> <p>Prepare your Github Token</p> </li> <li> <p>Install the Github python dependencies needed to generate the changelog.</p> <pre><code>pip install PyGithub\n</code></pre> </li> </ol>"},{"location":"release/changelog/#generate-release-notes","title":"Generate release notes","text":"<ol> <li> <p>Run the following command and fetch oneline git commits from the last release (v0.3.0) to current release (v0.4.0).</p> <pre><code>git log v0.3.0..v0.4.0 --oneline\n</code></pre> <p>You may need to run the following command first:</p> <pre><code>git fetch --tags\n</code></pre> </li> <li> <p>Copy the above commit history to <code>scripts/changelog-generator.py</code> and replace <code>&lt;your_github_token&gt;</code> with your Github token. Run the script to generate changelogs.</p> <pre><code>from github import Github\nimport re\n\n\nclass ChangelogGenerator:\n    def __init__(self, github_repo):\n        # Replace &lt;your_github_token&gt; with your Github Token\n        self._github = Github('&lt;your_github_token&gt;')\n        self._github_repo = self._github.get_repo(github_repo)\n\n    def generate(self, pr_id):\n        pr = self._github_repo.get_pull(pr_id)\n\n        return \"{title} ([#{pr_id}]({pr_link}), @{user})\".format(\n            title=pr.title,\n            pr_id=pr_id,\n            pr_link=pr.html_url,\n            user=pr.user.login\n        )\n\n\n# generated by `git log &lt;oldTag&gt;..&lt;newTag&gt; --oneline`\npayload = '''\n7374e2c [RayService] Skip update events without change (#811) (#825)\n7f83353 Switch to 0.4.0 and eliminate Chart app versions. (#810)\n86b0af2 Remove ingress.enabled from KubeRay operator chart (#812) (#816)\nc1cbaed Update chart versions for 0.4.0-rc.0 (#804)\n84a70f1 Update image tags. (#784)\nd760b9c [helm] Add memory limits and resource documentation. (#789) (#798)\n16905df [Feature] Improve the observability of integration tests (#775) (#796)\n83aab82 [CI] Pin go version in CRD consistency check (#794) (#797)\n....\n'''\n\ng = ChangelogGenerator(\"ray-project/kuberay\")\nfor pr_match in re.finditer(r\"#(\\d+)\", payload):\n    pr_id = int(pr_match.group(1))\n    print(\"* {}\".format(g.generate(pr_id)))\n</code></pre> </li> <li> <p>To create the release notes, save the output of the script. Modify the script's output as follows.</p> <ul> <li>Remove extraneous data, such as commits with tag information or links to other PRs, e.g.</li> </ul> <pre><code>- c1cbaed (tag: v0.4.0-rc.0) Update chart versions for 0.4.0-rc.0 (#804) -&gt; c1cbaed Update chart versions for 0.4.0-rc.0 (#804)\n- 86b0af2 Remove ingress.enabled from KubeRay operator chart (#812) (#816) -&gt; 86b0af2 Remove ingress.enabled from KubeRay operator chart (#816)\n</code></pre> <ul> <li>Group commits by category e.g. <code>KubeRay Operator</code>, <code>Documentation</code>, etc. (The choice of categories is at the release manager's discretion.)</li> <li>Add a section summarizing important changes.</li> <li>Add a section listing individuals who contributed to the release.</li> </ul> </li> <li> <p>Cut the release from tags and add the release notes from the last step. For an example, see the v0.3.0 release notes.</p> </li> <li> <p>Send a PR to update CHANGELOG.md. The changelog should be updated by prepending the new release notes.</p> </li> </ol>"},{"location":"release/helm-chart/","title":"Helm chart","text":""},{"location":"release/helm-chart/#helm-charts-release","title":"Helm charts release","text":"<p>We host all Helm charts on kuberay-helm. This document describes the process for release managers to release Helm charts.</p>"},{"location":"release/helm-chart/#the-end-to-end-workflow","title":"The end-to-end workflow","text":""},{"location":"release/helm-chart/#step-1-update-versions-in-chartyaml-and-valuesyaml-files","title":"Step 1: Update versions in Chart.yaml and values.yaml files","text":"<p>Please update the value of <code>version</code> in ray-cluster/Chart.yaml, kuberay-operator/Chart.yaml, and kuberay-apiserver/Chart.yaml to the new release version (e.g. 0.4.0).</p> <p>Also make sure <code>image.tag</code> has been updated in kuberay-operator/values.yaml and kuberay-apiserver/values.yaml.</p>"},{"location":"release/helm-chart/#step-2-copy-the-helm-chart-directory-from-kuberay-to-kuberay-helm","title":"Step 2: Copy the helm-chart directory from kuberay to kuberay-helm","text":"<p>In kuberay-helm CI, <code>helm/chart-releaser-action</code> will create releases for all charts in the directory <code>helm-chart</code> and update <code>index.yaml</code> in the gh-pages branch when the PR is merged into <code>main</code>. Note that <code>index.yaml</code> is necessary when you run the command <code>helm repo add</code>. I recommend removing the <code>helm-chart</code> directory in the kuberay-helm repository and creating a new one by copying from the kuberay repository.</p>"},{"location":"release/helm-chart/#step-3-validate-the-charts","title":"Step 3: Validate the charts","text":"<p>When the PR is merged into <code>main</code>, the releases and <code>index.yaml</code> will be generated. You can validate the charts as follows:</p> <ul> <li>Confirm that the releases are created as expected.</li> <li>Confirm that index.yaml exists.</li> <li>Confirm that index.yaml has the metadata of all releases, including old versions.</li> <li> <p>Check the creation/update time of all releases and <code>index.yaml</code> to ensure they are updated.</p> </li> <li> <p>Install charts from Helm repository.</p> <pre><code>helm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm repo update\n\n# List all charts\nhelm search repo kuberay\n\n# Install charts (If you want to install charts for a release candidate, add `--version vX.Y.Z-rc.0` to the command below.)\nhelm install kuberay-operator kuberay/kuberay-operator\nhelm install kuberay-apiserver kuberay/kuberay-apiserver\nhelm install raycluster kuberay/ray-cluster\n</code></pre> </li> </ul>"},{"location":"release/helm-chart/#delete-the-existing-releases","title":"Delete the existing releases","text":"<p><code>helm/chart-releaser-action</code> does not encourage users to delete existing releases; thus, <code>index.yaml</code> will not be updated automatically after the deletion. If you really need to do that, please read this section carefully before you do that.</p> <ul> <li>Delete the releases</li> <li> <p>Remove the related tags using the following command. If tags are not properly removed, you may run into the problem described in ray-project/kuberay/#561.</p> <pre><code># git remote -v\n# upstream        git@github.com:ray-project/kuberay-helm.git (fetch)\n# upstream        git@github.com:ray-project/kuberay-helm.git (push)\n\n# The following command deletes the tag \"ray-cluster-0.4.0\".\ngit push --delete upstream ray-cluster-0.4.0\n</code></pre> </li> <li> <p>Remove <code>index.yaml</code></p> </li> <li>Trigger kuberay-helm CI again to create new releases and a new index.yaml.</li> <li>Follow \"Step 3: Validate the charts\" to test it.</li> </ul>"}]}