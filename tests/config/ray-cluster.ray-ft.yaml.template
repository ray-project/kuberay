kind: ConfigMap
apiVersion: v1
metadata:
  name: redis-config
  labels:
    app: redis
data:
  redis.conf: |-
    dir /data
    port 6379
    bind 0.0.0.0
    appendonly yes
    protected-mode no
    requirepass 5241590000000000
    pidfile /data/redis-6379.pid
---
apiVersion: v1
kind: Service
metadata:
  name: redis
  labels:
    app: redis
spec:
  type: ClusterIP
  ports:
    - name: redis
      port: 6379
  selector:
    app: redis
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  labels:
    app: redis
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
        - name: redis
          image: redis:5.0.8
          command:
            - "sh"
            - "-c"
            - "redis-server /usr/local/etc/redis/redis.conf"
          ports:
            - containerPort: 6379
          volumeMounts:
            - name: config
              mountPath: /usr/local/etc/redis/redis.conf
              subPath: redis.conf
      volumes:
        - name: config
          configMap:
            name: redis-config
---
apiVersion: ray.io/v1alpha1
kind: RayCluster
metadata:
  labels:
    controller-tools.k8s.io: "1.0"
  annotations:
    ray.io/ft-enabled: "true" # enable Ray GCS FT
  name: raycluster-external-redis
spec:
  rayVersion: '$ray_version'
  headGroupSpec:
    serviceType: ClusterIP
    replicas: 1
    rayStartParams:
      dashboard-host: '0.0.0.0'
      num-cpus: '1'
      block: 'true'
    #pod template
    template:
      metadata:
        labels:
          # Enables Ray client command execution via the NodePort service
          # configured in tests/config/raycluster-service.yaml.
          ray.io/test: compatibility-test-label
      spec:
        containers:
          - name: ray-head
            image: $ray_image
            env:
              # RAY_REDIS_ADDRESS can force ray to use external redis
              - name: RAY_REDIS_ADDRESS
                value: redis:6379
            ports:
              - containerPort: 6379
                name: redis
              - containerPort: 8265
                name: dashboard
              - containerPort: 10001
                name: client
            livenessProbe:
              initialDelaySeconds: 30
              periodSeconds: 3
              timeoutSeconds: 1
              failureThreshold: 400
            readinessProbe:
              initialDelaySeconds: 30
              periodSeconds: 3
              timeoutSeconds: 1
              failureThreshold: 300
            volumeMounts:
              - mountPath: /home/ray/samples
                name: test-script-configmap
        volumes:
          - name: test-script-configmap
            configMap:
              name: test-script
              # An array of keys from the ConfigMap to create as files
              items:
                - key: test_ray_serve_1.py
                  path: test_ray_serve_1.py
                - key: test_ray_serve_2.py
                  path: test_ray_serve_2.py
                - key: test_detached_actor_1.py
                  path: test_detached_actor_1.py
                - key: test_detached_actor_2.py
                  path: test_detached_actor_2.py
  workerGroupSpecs:
    # the pod replicas in this group typed worker
    - replicas: 2
      minReplicas: 1
      maxReplicas: 2
      # logical group name, for this called small-group, also can be functional
      groupName: small-group
      rayStartParams:
        block: 'true'
      #pod template
      template:
        spec:
          initContainers: # to avoid worker crashing before head service is created
            - name: init
              image: busybox:1.28
              command: ['sh', '-c', "until nslookup $$RAY_IP.$$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for K8s Service $$RAY_IP; sleep 2; done"]
          containers:
            - name: ray-worker # must consist of lower case alphanumeric characters or '-', and must start and end with an alphanumeric character (e.g. 'my-name',  or '123-abc'
              image: $ray_image
              resources:
                limits:
                  cpu: "1"
                requests:
                  cpu: "200m"
              livenessProbe:
                initialDelaySeconds: 30
                periodSeconds: 3
                timeoutSeconds: 1
                failureThreshold: 400
              readinessProbe:
                initialDelaySeconds: 30
                periodSeconds: 3
                timeoutSeconds: 1
                failureThreshold: 300
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: test-script
data:
  test_ray_serve_1.py: |
    import requests
    from starlette.requests import Request
    import ray
    from ray import serve
    import time
    import sys

    # 1: Define a Ray Serve model.
    @serve.deployment(route_prefix="/")
    class MyModelDeployment:
        def __init__(self, msg: str):
            self._msg = msg

        def __call__(self):
            return self._msg

    ray.init(namespace=sys.argv[1])
    # 2: Deploy the model.
    handle = serve.run(MyModelDeployment.bind(msg="Hello world!"))
    # 3: Query the deployment and print the result.
    val = ray.get(handle.remote())
    print(val)
    assert(val == "Hello world!")

  test_ray_serve_2.py: |
    import ray
    import time
    import sys
    from ray import serve

    def retry_with_timeout(func, timeout=90):
        err = None
        for _ in range(timeout):
            try:
                return func()
            except BaseException as e:
                err = e
            finally:
                time.sleep(1)
        raise err

    retry_with_timeout(lambda: ray.init(namespace=sys.argv[1]))
    retry_with_timeout(lambda:serve.start(detached=True))
    val = retry_with_timeout(lambda: ray.get(serve.get_deployment("MyModelDeployment").get_handle().remote()))
    print(val)
    assert(val == "Hello world!")

  test_detached_actor_1.py: |
    import ray
    import sys

    ray.init(namespace=sys.argv[1])

    @ray.remote
    class TestCounter:
        def __init__(self):
            self.value = 0

        def increment(self):
            self.value += 1
            return self.value

    tc = TestCounter.options(name="testCounter", lifetime="detached", max_restarts=-1).remote()
    val1 = ray.get(tc.increment.remote())
    val2 = ray.get(tc.increment.remote())
    print(f"val1: {val1}, val2: {val2}")

    assert(val1 == 1)
    assert(val2 == 2)

  test_detached_actor_2.py: |
    import ray
    import time
    import sys

    def retry_with_timeout(func, timeout=90):
        err = None
        start = time.time()
        while time.time() - start <= timeout:
            try:
                return func()
            except BaseException as e:
                err = e
            finally:
                time.sleep(1)
        raise err

    def get_detached_actor():
        return ray.get_actor("testCounter")

    # Try to connect to Ray cluster.
    print("Try to connect to Ray cluster.")
    retry_with_timeout(lambda: ray.init(namespace=sys.argv[1]), timeout = 180)

    # Get TestCounter actor
    print("Get TestCounter actor.")
    tc = retry_with_timeout(get_detached_actor)

    print("Try to call remote function \'increment\'.")
    val = retry_with_timeout(lambda: ray.get(tc.increment.remote()))
    print(f"val: {val}")

    # The actual value should be 1 rather than 2. Ray will launch all registered actors when
    # the ray cluster restarts, but the internal state of the state will not be restored.
    assert(val == 1)
